{
  "agent_id": "coder2",
  "task_id": "task_6",
  "files": [
    {
      "name": "setup.py",
      "purpose": "Package installation setup",
      "priority": "low"
    }
  ],
  "project_info": {
    "project_name": "enhanced_stat.ML_2508.18037v1_Enhancing_Differentially_Private_Linear_Regression",
    "project_type": "agent",
    "description": "Enhanced AI project based on stat.ML_2508.18037v1_Enhancing-Differentially-Private-Linear-Regression with content analysis. Detected project type: agent (confidence score: 3 matches).",
    "key_algorithms": [
      "Standard",
      "Pac",
      "Machine",
      "Data",
      "Truncation",
      "Transformation",
      "True",
      "Chine",
      "Privately",
      "Ized"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: stat.ML_2508.18037v1_Enhancing-Differentially-Private-Linear-Regression.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nEnhancing Differentially Private Linear Regression via\nPublic Second-Moment\nZilong Caoa, Hai Zhanga,\u2217\naThe School of Mathematics, Northwest University, Xi\u2019an, 710127, Shaanxi, China\nAbstract\nLeveraging information from public data has become increasingly crucial in\nenhancing the utility of differentially private (DP) methods. Traditional DP\napproaches often require adding noise based solely on private data, which can\nsignificantly degrade utility. In this paper, we address this limitation in the\ncontext of the ordinary least squares estimator (OLSE) of linear regression\nbased on sufficient statistics perturbation (SSP) under the unbounded data\nassumption. We propose a novel method that involves transforming private\ndata using the public second-moment matrix to compute a transformed SSP-\nOLSE, whose second-moment matrix yields a better condition number and\nimproves the OLSE accuracy and robustness. We derive theoretical error\nbounds about our method and the standard SSP-OLSE to the non-DP OLSE,\nwhich reveal the improved robustness and accuracy achieved by our approach.\nExperiments on synthetic and real-world datasets demonstrate the utility and\neffectiveness of our method.\nKeywords:\nDifferential privacy, public data, linear regression, second-moment\n1. Introduction\nAs privacy concerns gain increasing attention in society, protecting in-\ndividuals\u2019 sensitive information has become essential in data science. It is\ncrucial to assess the risk of privacy leaks when utilizing data. Differential\n\u2217Corresponding author\nEmail addresses: nwu_czl@stumail.nwu.edu.cn (Zilong Cao),\nzhanghai@nwu.edu.cn (Hai Zhang)\nPreprint submitted to Arxiv August 26, 2025arXiv:2508.18037v1  [cs.LG]  25 Aug 2025\n\n--- Page 2 ---\nprivacy (DP), introduced by [1], provides a well-defined mathematical frame-\nwork that ensures the output of a DP algorithm is unsensitive to individual\nchanges, preventing attackers from inferring information specific to any indi-\nvidual. DP has found applications in statistics [2, 3, 4, 5], machine learning\n[6, 7, 8, 9], and other fields.\nDP algorithms rely on random mechanisms that introduce noise based\non private data to obscure individual contributions. However, the mecha-\nnisms based solely on private data often yield highly unstable results and\npoor utility. The public data, as valid and privacy-free information, exist in\nmany areas, but the previous DP methods are always hard to use. Recently,\nseveral studies have explored leveraging public data to enhance both privacy\nprotection and utility in DP methods, which has thus become an important\narea of research. For instance, Ferrando et al. [10] examines how to combine\npublic and private data for improved statistical estimation, while Bie et al.\n[11] shows that even a small amount of public data can improve Gaussian\ndistribution estimation. Nasr et al. [12] proposes an effective DP gradient\nalgorithm that enhances machine learning models using public data, with\nsimilar studies in this area, including [13, 14]. Additional researches have ex-\nplored applications in private query release, synthetic data generation, and\nprediction [15, 16, 17, 18].\nPrior research on differentially private (DP) linear regression primarily\nfalls into three categories: gradient perturbation methods [19, 20], objec-\ntive perturbation [21, 22], and sufficient statistics perturbation (SSP) [23].\nAmong them, Wang [23] systematically investigates SSP under bounded pri-\nvate data and demonstrates that it outperforms alternative approaches in\nboth utility and efficiency. Several follow-up works further explore the SSP\nframework. For instance, Sheffet [24] employs the Johnson-Lindenstrauss\nTransform (JLT) to enhance statistical inference and hypothesis testing.\nThis method applies a global, data-dependent random projection to the en-\ntire dataset, introducing privacy while preserving key structural informa-\ntion. Milionis et al. [25] consider a Bayesian perspective under Gaussian\nassumptions, where the DP-OLSE inherits additional theoretical bias due to\nprivatized second-moment matrices and the choice of tuning parameters. De-\nspite these advancements, existing works rely solely on private data, which\npresents several limitations: (i) they struggle with unbounded data, lead-\ning to unbounded sensitivities and excessive noise; (ii) utility improvements\nare inherently limited when only private information is used; and (iii) the\nresulting estimators often suffer from numerical instability.\n2\n\n--- Page 3 ---\nIn this paper, we focus on the ordinary least squares estimator (OLSE) of\nlinear regression based on the sufficient statistics perturbation (SSP) un-\nder the assumptions of available public second-moment matrix and data\ndistributed with sub-Gaussian which includes bounded and many classes\nof unbounded distributions. It is well known that the closed-form solu-\ntion to the ordinary least squares (OLS) problem is given by (X\u22a4X\nn)\u22121X\u22a4y\nn,\nwhich crucially depends on the empirical second-moment matrix of the fea-\ntures, \u02c6\u03a3 =1\nnX\u22a4X, and its inverse, \u02c6\u03a3\u22121. In the DP setting, noise perturba-\ntion\u2014denoted by G\u2014added to \u02c6\u03a3 results in the perturbed matrix ( \u02c6\u03a3 +G),\nwhose inverse ( \u02c6\u03a3 +G)\u22121is often numerically unstable and inaccurate. This\ninstability is especially pronounced when \u02c6\u03a3 is ill-conditioned, as a high condi-\ntion number exacerbates the sensitivity of matrix inversion to perturbations,\nthereby significantly impairing the accuracy of the resulting regression es-\ntimates. To address the challenge, we introduce a public second-moment\ntransformation framework for differentially private linear regression. We\ntransform the private data using \u02c6\u03a3puband obtain a DP linear regression\nestimator based on the transformed data, ultimately recovering the original\nprivate estimation through \u02c6\u03a3pub. The public second-moment transformation\ndecreases the condition number of the features\u2019 second-moment matrix and\nimproves its numerical stability, hence the accuracy of DP regression. Ex-\nperiments demonstrate that our method is more effective and stable than the\nmethod relying solely on private data, with our theoretical analysis guaran-\nteeing error bound and robustness.\nOur Contributions:\n\u2022We propose a novel data transformation technique leveraging a public\nsecond-moment matrix, which improves the effect of data truncation,\nreduces data sensitivity, and enhances the stability and accuracy of\nDP-OLSE. Furthermore, we demonstrate that the transformation is\nreversible for OLSE, ensuring that it does not alter the original OLSE.\n\u2022We analyze the stability condition of the perturbed inverse second-\nmoment matrix and reduce the requirement for the size of private data\nfrom O(d3/2log(1\n\u03b7)\n\u221a\u03c1\u00b7n\u0010\n\u00af\u03ba(\u03a3)+\u2225\u03a3\u22121\u2225log(2n\n\u03b7)\u0011\n) toO(d3/2log(1\n\u03b7)\n\u221a\u03c1\u00b7nlog(2n\n\u03b7)), mak-\ning stability independent of the (private) second-moment matrix \u03a3.\n\u2022We guarantee our method\u2019s DP estimator bounded by O\u0010d3/2\u2225\u03b2\u2225\u03ba(\u03a3) log(1\n\u03b7)\n\u221a\u03c1\u00b7nlog(2n\n\u03b7)\u0011\n3\n\n--- Page 4 ---\nrather than the non-public bound O\u0010d3/2\u2225\u03b2\u2225\u03ba(\u03a3) log(1\n\u03b7)\n\u221a\u03c1\u00b7n\u00b7\u0010\nlog(2n\n\u03b7)\u2225\u03a3\u22121\u2225+\n\u00af\u03ba(\u03a3)\u0011\u0011\n. Our result eliminates the impacts of the averaged condition\nnumber and the norm of the inverse second-moment matrix signifi-\ncantly.\nThis paper is organized as follows. We state our motivation, some fun-\ndamental definitions, and useful tools in Section 2. Section 3 is main in this\npaper. We give all algorithms and theoretical results. Typically, Subsection\n3.4 shows the theoretical explanations why our method is advantageous. Sec-\ntion 4 shows the experiments on the synthetic data and real data. Section 5\ngives the conclusion and future work.\n2. Problem Statement and Motivation\nIn this section, we firstly remind you of the privacy-protecting concepts\nand tools in Subsection 2.1. Then, Subsection 2.2 states the standard DP lin-\near regression, our setting, and their existing issues. Subsection 2.3 proposes\nthe motivation for how to cope with the existing issues via leveraging the\npublic second-moment matrix. Finally, Subsection 2.4 gives all the crucial\nanalogous tools and explains their effects.\n2.1. Background about DP\nDifferential Privacy (DP) provides a rigorous framework to quantify pri-\nvacy in statistical learning. In this subsection, we briefly review the core\nconcepts and tools used throughout this work, with a focus on those relevant\nto private linear regression.\nDefinition 1 (Differential Privacy[1]) .A randomized algorithm M:Xn\u2192 S\nsatisfies (\u03f5, \u03b4)\u2212differential privacy ( (\u03f5, \u03b4)-DP), if for any neighboring datasets\nX,X\u2032\u2208 Xn(they differ in only one sample) and \u2200S\u2286 S, \u03f5 > 0, \u03b4 > 0, the\nfollowing probability inequality hold\nP[M(X)\u2208S]\u2264exp(\u03f5)P[M(X\u2032)\u2208S] +\u03b4,\nwhen \u03b4= 0means \u03f5-DP.\nThis definition characterizes the indistinguishability of outputs under\nneighboring datasets. DP possesses two important privacy-protecting proper-\nties: (i) post-processing . The post-operations solely based on a DP algorithm\n4\n\n--- Page 5 ---\nwill not increase the risk of private leakages; namely, if a DP algorithm\nM(D) satisfies ( \u03f5, \u03b4)-DP, a post-operation M\u2032(M(D)) still satisfies ( \u03f5, \u03b4)-\nDP. (ii) Composition theorem . For n(\u03f5i, \u03b4i)-DP algorithms Mi, i\u2208[n],\ntheir composition M(D) = (M1(D),\u00b7\u00b7\u00b7,Mn(D)) is (Pn\ni=1\u03f5i,Pn\ni=1\u03b4i)-DP.\nNext, we introduce the zero-concentrated differential private (zCDP), which\nis an variant of DP and simplify the two privacy parameters ( \u03f5, \u03b4) to one\nparameter \u03c1. zCDP will serve as our primary privacy accounting tool.\nDefinition 2 (Zero-Concentrated Differential Private (zCDP)) .A random-\nized algorithm M:Xn\u2192 S satisfies \u03c1\u2212zCDP, if for any neighboring datasets\nX,X\u2032\u2208 Xn(they differ in only one sample),\n\u2200\u03b1\u2208(1,\u221e), D \u03b1(M(X)||M(X\u2032))\u2264\u03c1\u03b1,\nwhere D\u03b1(M(X)||M(X\u2032))is the \u03b1-Renyi divergence.\nzCDP implies ( \u03b5, \u03b4)-DP and inherits all the properties of DP.\nLemma 1 (zCDP and DP[26]) .A randomized algorithm M:Xn\u2192 S\nsatisfies \u03c1-zCDP. Then Msatisfies (\u03c1+ 2p\n\u03c1log(1 /\u03b4), \u03b4)-DP for \u2200\u03b4 >0.\nLemma 2 (zCDP composition[26]) .Random algorithms M1(X), ..., M T(X)\nsatisfy \u03c1i-zCDP, respectively, then M(X) = ( M1(X), ..., M T(X))satisfiesPT\ni=1\u03c1i.\nThe Gaussian mechanism is a standard approach for achieving zCDP. Its\ndesign relies on the concept of sensitivity , which quantifies the maximum\nchange in the algorithm\u2019s output resulting from the modification of a single\ndata point. To ensure privacy, Gaussian noise is added proportionally to this\nsensitivity. However, the sensitivity typically depends on the range or norm\nbounds of the input data. As a result, unbounded or widely dispersed data\npose a significant challenge for differentially private algorithms, often leading\nto excessive noise and degraded utility.\nDefinition 3 (Sensitivity) .Letf:Xn\u2192Rdbe an algorithm, its l2\u2212\nsensitivity is\n\u2206f= max\nX\u223cX\u2032\u2208Xn\u2225f(X)\u2212f(X\u2032)\u22252,\nwhere X\u223cX\u2032means that they are neighboring datasets, which differ in only\none sample.\n5\n\n--- Page 6 ---\nLemma 3 (Gaussian mechanism) .Letf:Xn\u2192Rdbe a function with\nsensitivity \u2206f. Then the Gaussian mechanism\nM(X) =f(X) +N\u0010\n0,\u0010\u2206f\u221a2\u03c1\u00112\nId\u00d7d\u0011\nsatisfies \u03c1-zCDP.\n2.2. Private Linear Regression: Setup and Challenges\nWe consider the Linear Regression as\ny=X\u03b2+\u03f5,\nwhere Xis an\u00d7ddata matrix and X= [x\u22a4\n1,\u00b7\u00b7\u00b7,x\u22a4\nn]\u22a4,xi\u2208Rd\u00d71is the i\u2212th\nsample. yis the respond variable vector. The noise vector \u03f5\u223c N(0, \u03c32In\u00d7n).\nIn this paper, we consider a setting where there are private data matrix\nAwith nAsamples and public data matrix Bwith nBsamples, and every\nsample is the i.i.drandom from the sub-gaussian distribution subG (\u03a3) with\nthe second-moment matrix \u03a3 and nA, nB> d. When the private data is used\nto get the ordinary least square estimator(OLSE),\n\u02c6\u03b2A=\u0010A\u22a4A\nnA\u0011\u22121A\u22a4yA\nnA.\nFor privacy protection, we achieve the differentially private ordinary least\nsquare estimator (DP-OLSE) via the sufficient statistics perturbation (SSP),\n\u02c6\u03b2DP\nA=\u0010A\u22a4A\nnA+G\u0011\u22121\u0010A\u22a4yA\nnA+g\u0011\n,\nwhere Gis the DP perturbation matrix, gis the DP perturbation vector\nandyAis corresponding private respond vector. Our goal is to obtain a dif-\nferentially private estimator \u02c6\u03b2DP\nA, even though the distribution of the private\ndata is unknown. The standard SSP approach consists of four steps: (i)\ntruncating the data to ensure boundedness, (ii) computing the relevant suffi-\ncient statistics and their sensitivities, (iii) perturbing these statistics via the\nGaussian mechanism, and (iv) solving for a differentially private ordinary\nleast squares estimator (DP-OLSE).\nHowever, the standard SSP suffers from two major drawbacks:\n6\n\n--- Page 7 ---\n\u2022Sensitivity & Truncation. The unbounded data lead to uncon-\ntrolled sensitivities for bothA\u22a4A\nnAandA\u22a4yA\nnA, making the added noise\noverly large and the resulting DP estimates unreliable. This raises\nthe first key challenge: determining an appropriate truncation radius.\nWithout prior knowledge of the scale of the data vectors, truncation\ncan either distort the underlying distribution\u2014yielding a biased and\ninvalid OLSE\u2014or result in overly conservative bounds, introducing ex-\ncessive DP noise and severely degrading accuracy.\n\u2022Numerical Instability. In high dimensions or near-singular settings,\ntheA\u22a4A\nnAmay become ill-conditioned, causing instability in second-\nmoment matrix inversion and consequently, in the final DP regression\noutput.\n2.3. Motivation: How Public Second-moment Can Help\n2.3.1. Transformation via Public Second Moment\nAn isotropic sub-Gaussian random vector z\u2208Rdsatisfies the high-\nprobability bound \u2225z\u22252\n2\u2264O\u0000\nd(1+log(2\n\u03b7))\u0001\n, which implies that with probabil-\nity at least 1 \u2212\u03b7,zlies within a Euclidean ball of radius R=O\u0000q\nd(1 + log(2\n\u03b7))\u0001\n.\nThis motivates the choice of such a truncation radius as a reasonable trade-\noff between preserving data utility and limiting the magnitude of injected DP\nnoise. However, in practical scenarios, the data distribution is typically non-\nisotropic, with an unknown second-moment matrix \u03a3. A natural strategy\nto address this anisotropy is to apply a linear transformation that approx-\nimately \u201cwhitens\u201d the data. Specifically, we consider finding a symmetric\npositive-definite matrix Msuch that the transformed vector \u02dcz=M\u22121/2zis\ncloser to isotropic\nE\u02dcz\u02dcz\u22a4=M\u22121/2\u03a3M\u22121/2\u2248I.\nIn addition, the second-moment matrix of the transformed data, E[\u02dcz\u02dcz\u22a4],\nbecomes well-conditioned, typically satisfying \u03ba(E[\u02dcz\u02dcz\u22a4])\u22481. A small con-\ndition number is essential for ensuring the stability and accuracy of inverse\nsecond-moment matrix estimation. This is particularly critical in the con-\ntext of DP-OLSE, where random noise is inevitably added to the estimated\nsecond-moment matrix. When the original matrix is ill-conditioned, such\nperturbations can significantly render the inverse unreliable and amplify es-\ntimation errors, leading to severely degraded regression performance. To\n7\n\n--- Page 8 ---\naddress this, we shall demonstrate that \u02c6\u03a3pubserves as an effective precondi-\ntioning matrix, yielding transformed data that are approximately isotropic\nand thereby facilitating more robust and accurate DP linear regression.\n2.3.2. Affine Invariance of OLSE\nWe state that the transformation doesn\u2019t change the original OLSE. De-\nnoteA\u2208RnA\u00d7das the private data matrix and B\u2208RnB\u00d7das the public data\nmatrix, \u02c6\u03a3B=1\nnBPnB\ni=1B\u22a4Bas the second-moment matrix estimation from\npublic data Band \u02c6\u03c3B=q\n1\nnBPnB\ni=1y2\nias the second-moment estimation of\nyB. Then, we transform the OLSE of private data as\n\u02dc\u03b2A=\u02c6\u03a31/2\nB\n\u02c6\u03c3B\u02c6\u03b2A= (\u02c6\u03a3\u22121/2\nBA\u22a4A\u02c6\u03a3\u22121/2\nB)\u22121\u02c6\u03a3\u22121/2\nBA\u22a4yA\n\u02c6\u03c3B\n(i)=\u0010\u02dcA\u22a4\u02dcA\nnA\u0011\u22121\u0010\u02dcA\u22a4\u02dcyA\nnA\u0011\n(ii)= (\u02dc\u03a3A)\u22121\u02dc\u03a3Ay,(1)\nwhere \u02dcA=A\u02c6\u03a3\u22121/2\nBand\u02dcyA=yA\n\u02c6\u03c3Bin Eq.(i), and \u02dc\u03a3A=\u02dcA\u22a4\u02dcA\nnAand\u02dc\u03a3Ay=\u02dcA\u22a4\u02dcyA\nnAin Eq.(ii). Eq.(1) means that we can translate discussion from the original\nOLSE \u02c6\u03b2Ato the transformed OLSE \u02dc\u03b2Atotally. The remainder of the paper\nformalizes this transformation, analyzes its privacy and utility properties,\nand proposes practical DP mechanisms based on it.\n2.4. Useful Tools\nThe subsection introduces the analogous tools. Lemma 4 shows the\nlength of a sub-Gaussian sample is bounded by the trance of the second-\nmoment matrix and the dimension, which provides a principled truncation\nradius.\nLemma 4 (Concentration of the sub-exponential norm) .Letx= (x1, ...x d)\u22a4\u2208\nRdis a non-isotropic sub-gaussian random vector with Exx\u22a4= \u03a3. Then \u2225x\u22252\n2\nis sub-exponential and\nPh\f\f\u2225x\u22252\n2\u2212dX\ni=1\u03bbi| \u2265ti\n\u22642 exp\u0000\n\u2212ct\ndK2\u0001\n, (2)\n8\n\n--- Page 9 ---\nwhere K=max i\u2225xi\u2225\u03c82,cis an absolute constant on Kand\u03bb1> ... > \u03bb d\nare the eigenvalues of \u03a3. One more thing, with at least probability 1\u2212\u03b7,\n\u2225x\u22252\n2\u2264Tr(\u03a3) +\u0000dK2\nclog(2\n\u03b7)\u0001\n=d\u0000\nTr(\u03a3) +K2\nclog(2\n\u03b7)\u0001\n,\nwhere Tr(\u03a3) =1\ndPd\ni\u03bbi(\u03a3)is the average of the traces.\nProof 1. For simplicity, we assume that K\u22651. Since the random xiis\nsub-gaussian, x2\niis sub-exponential, and more precisely\n\n\n\u2225x\u22252\n2\n\n\u03c81=\u2225dX\ni=1x2\ni\u2225\u03c81\n\u2264dX\ni=1\u2225x2\ni\u2225\u03c81\n\u2264dmax\ni\u2225x2\ni\u2225\u03c81\n=dmax\ni\u2225xi\u22252\n\u03c82.\nThen, we compute the expectation of \u2225x\u22252\n2\nE\u2225x\u22252\n2=Ex\u22a4x\n=ETr(x\u22a4x)\n=ETr(xx\u22a4)\n= Tr(Exx\u22a4)\n= Tr(\u03a3)\n=dX\ni=1\u03bbi.\nReconsider the tail bound of the centered sub-exponential random, we have\nPh\f\f\f\u2225x\u22252\n2\u2212E\u2225x\u22252\n2\f\f\f> ti\n\u22642 exp\u0000\n\u2212ct\ndK2\u0001\n,\nwhere K= max i\u2225xi\u2225\u03c82andcis an absolute constant. \u25a1\nThe next lemma gives the largest and smallest singular values of the\nsub-Gaussian matrix. It helps us to analyze the second-moment matrix esti-\nmation and the OLSE.\n9\n\n--- Page 10 ---\nLemma 5 (Singular values bound[27]) .LetXbe a n\u00d7drandom matrix\nwhose each row xiis independently non-isotropic sub-gaussian random vec-\ntors in Rdwith the second-moment matrix \u03a3. Then for every \u03b7 >0, with at\nleast 1\u22122\u03b7, one has\np\nn\u03bbmin(\u03a3)\u2212C\u221a\nd\u2212r\n1\nclog(1 /\u03b7)\u2264\u03bbmin(X),\np\nn\u03bbmax(\u03a3) + C\u221a\nd+r\n1\nclog(1 /\u03b7)\u2265\u03bbmax(X),(3)\nwhere \u03bb(\u00b7)means singular value, and C=CK,c=cK>0depend on the\nsub-gaussian norm K=max i\u2225xi\u2225\u03c82. Typically, for the isotropic situation\n\u03a3 =I, one has\n\u221an\u2212C\u221a\nd\u2212r\n1\nclog(1 /\u03b7)\u2264\u03bbmin(X),\n\u221an+C\u221a\nd+r\n1\nclog(1 /\u03b7)\u2265\u03bbmax(X).(4)\nCorollary 1. LetXbe an\u00d7drandom matrix whose each row xiis indepen-\ndently isotropic sub-gaussian random vector in Rdwith the second-moment\nmatrix \u03a3 =I. Then for every \u03b7 >0, with at least 1\u2212O(\u03b7), one has\nP\u0002\u03bbmax(X\u22a4X)\nn=\u03bb2\nmax(X)\nn\u2265\u0010\n1 +O\u0010r\nd\nn+r\nlog(1 /\u03b7)\nn\u0011\u00112\u0003\n\u2264\u03b7\nand\nP\u0002\u03bbmin(X\u22a4X)\nn=\u03bb2\nmin(X)\nn\u2264\u0010\n1\u2212O\u0010r\nd\nn+r\nlog(1 /\u03b7)\nn\u0011\u00112\u0003\n\u2264\u03b7.\nThe following lemma is our key tool to analyze the inverse noisy second-\nmoment matrix. The lemme includes the stable condition and the bound of\nthe perturbed inverse bound. These will provide the theoretical insights and\nexplanations why our method is more stable and accurate.\nLemma 6. Denote a square matrix \u03a3and a disturb matrix G, the condition\nnumber of \u03ba(\u03a3) = \u2225\u03a3\u2225\u2225\u03a3\u22121\u2225. Then,\n\u2225(\u03a3 +G)\u22121\u2212\u03a3\u22121\u2225\n\u2225(\u03a3 +G)\u22121\u2225\u2264\u03ba(\u03a3)\u2225G\u2225\n\u2225\u03a3\u2225.\n10\n\n--- Page 11 ---\nMoreover, if \u03ba(\u03a3)\u2225G\u2225\n\u2225\u03a3\u2225=\u2225\u03a3\u22121\u2225\u2225G\u2225 \u22641, then\n\u2225(\u03a3 +G)\u22121\u2225 \u2264\u2225\u03a3\u22121\u2225\n1\u2212\u03ba(\u03a3)\u2225G\u2225\n\u2225\u03a3\u2225. (5)\nMoreover\n\u2225(\u03a3 +G)\u22121\u2212\u03a3\u22121\u2225\n\u2225\u03a3\u22121\u2225\u2264\u03ba(\u03a3)\u2225G\u2225\n\u2225\u03a3\u2225\n1\u2212\u03ba(\u03a3)\u2225G\u2225\n\u2225\u03a3\u2225. (6)\nThere some facts about Gaussian random and Gaussian matrix. The d\u00d7d\nsymmetric Gaussian matrix Wwith entries Wiji.i.d.\u223c N (0, \u03c32) is denoted\nbyW\u223cGUE (\u03c32). These are used to analyze the impacts from the DP\nperturbations. The following lemmas show their concentration,\nLemma 7 (The symmetric Gaussian matrix bound[28]) .For a d\u00d7dsym-\nmetric matrix W\u223cGUE (\u03c32), there exist constants C, c\u22650such that\nP[\u2225M\u22252\u2265A\u03c3\u221a\nd]\u2264Cexp(\u2212cAd) (7)\nfor all A\u2265C. Or, with probability 1\u2212\u03b7,\n\u2225M\u22252\u2264O\u0010\n\u03c3\u221a\ndlog(1 /\u03b7)\u0011\n. (8)\nLemma 8 (Gaussian vector bound[29]) .For a gaussian vector x= (x1, ..., x d)\n, xi\u223c N(0, \u03c32), the\u2225x\u22252\n2satisfies\nP[\u2225x\u22252\n2\u2265\u03c32(2p\ndlog(1 /\u03b7) + 2 log(1 /\u03b7) +d)]\u2264\u03b7. (9)\n3. Differentially Private Linear Regression via Public Second-moment\nSubsection 3.1 introduces a transformation and truncation framework\nthat leverages the public second-moment matrix. We provide theoretical\njustification for the advantages of this approach, including bounded eigen-\nvalues of the transformed second-moment matrix and the better utility of\ntruncation. Next, Subsection 3.2 guarantees the transformed second-moment\nmatrix satisfying DP. Then, Subsection 3.3 integrates these components and\npresents our main algorithm for computing the differentially private regres-\nsion estimator, along with a formal guarantee of its privacy and utility. Fi-\nnally, to highlight the benefits of our method, we compare its theoretical\nperformance with that of the standard DP ordinary least squares estimator\n(DP-OLSE).\n11\n\n--- Page 12 ---\n3.1. Truncating Private Data Based on Public Second-moment\nThe algorithm 1 shows the details of truncating private data via the\nsecond-moment matrix estimation from public data. Generally, the algorithm\ncan be relaxed to just only a known public second moment.\nAlgorithm 1 Public-moment-transformed Truncation (PMT)\n1:Input: Private dataset {\u03bei}n\u03be\ni=1, public second-moment \u02c6\u03a3 =\n1\nn\u03c5Pn\u03c5\ni=1\u03c5\u03c5\u22a4, parameters d,n\u03beand\u03b7.\n2:Transform private data: \u02dc\u03bei=\u02c6\u03a3\u22121/2\u03bei, i= 1, ..., n \u03be.\n3:Truncating data: for every transformed data \u02dc\u03bei, i= 1, ..., n \u03be,\n4:while i\u2208[n\u03be]do\n5:if\u2225\u02dc\u03bei\u22252\u2265q\nd(1 + log(2n\u03be\n\u03b7))then\n6: \u02dc\u03bei\u2190q\nd(1 + log(2n\u03be\n\u03b7))\u00b7\u02dc\u03bei\n\u2225\u02dc\u03bei\u22252,\n7:else\n8: \u02dc\u03beiis itself.\n9:end if\n10:end while\n11:Output: Truncated dataset {\u02dc\u03bei}n\u03be\ni=1.\nThe following theorem shows the second-moment matrix of the trans-\nformed data possesses bounded eigenvalues tending to 1, which means it is\nmore well-conditioned and robust than the original second-moment matrix.\nThat also results in the a better truncation as the following corollary.\nTheorem 1 (Bound the second-moment matrix) .Denote that a random\nvector \u03be\u2208Rd\u00d71\u223csubG (\u03a3), where \u03a3 =E\u03be\u03be\u22a4is the second-moment matrix.\nSuppose \u03a5\u2208Rn\u00d7dis a data matrix whose samples are from subG (\u03a3), i.i.d.\nand\u02c6\u03a3 =1\nn\u03a5\u22a4\u03a5is an estimation of the second-moment matrix. Then \u02dc\u03be=\n\u02c6\u03a3\u22121/2\u03be\u223csubG (\u02c6\u03a3\u22121/2\u03a3\u02c6\u03a3\u22121/2)and, with at least probability 1\u22122\u03b7,\nLI\u2aaf\u02c6\u03a3\u22121/2\u03a3\u02c6\u03a3\u22121/2\u2aafUI, (10)\nwhere L=n\n(\u221an+O(\u221a\nd+q\n2 log(1\n\u03b7)))2andU=n\n(\u221an\u2212O(\u221a\nd+q\n2 log(1\n\u03b7)))2.\nProof 2. The goal equation is equal to\nL\u03a3\u22121/2\u02c6\u03a3\u03a3\u22121/2\u2aafI\u2aafU\u03a3\u22121/2\u02c6\u03a3\u03a3\u22121/2. (11)\n12\n\n--- Page 13 ---\nThen,\n\u03a3\u22121/2\u02c6\u03a3\u03a3\u22121/2=1\nnnX\ni=1(\u03a3\u22121/2\u03c5i)(\u03a3\u22121/2\u03c5i)\u22a4\n=1\nnnX\ni=1\u02dc\u03c5i\u02dc\u03c5i\u22a4\n=1\nn\u02dc\u03a5\u22a4\u02dc\u03a5=\u02dc\u03a3,\nwhere \u02dc\u03c5i\u223csubG (I), so\u02dc\u03a3 =1\nn\u02dc\u03a5\u22a4\u02dc\u03a5is an estimation of I. That means that\nI\u2aafU\u03a3\u22121/2\u02c6\u03a3\u03a3\u22121/2\u21d0\u21d2 1\u2264U\u03bbmin(\u02dc\u03a3),\nL\u03a3\u22121/2\u02c6\u03a3\u03a3\u22121/2\u2aafI\u21d0\u21d2 1\u2265L\u03bbmax(\u02dc\u03a3).\nFrom the Corollary 1 , we know\nU=\u0010\n1\u2212O\u0010r\nd\nn+r\nlog(1 /\u03b7)\nn\u0011\u0011\u22122\n=\u21d21\u2264U\u03bbmin(\u02dc\u03a3),\nL=\u0010\n1 +O\u0010r\nd\nn+r\nlog(1 /\u03b7)\nn\u0011\u0011\u22122\n=\u21d21\u2265L\u03bbmax(\u02dc\u03a3).\n\u25a1\nCorollary 2 (Untility of truncation) .Denote that random vectors \u03bei\u2208\nRd\u00d71i.i.d.\u223csubG (\u03a3), i= 1, ..., n \u03be, where \u03a3 =E\u03be\u03be\u22a4is the second-moment\nmatrix. Suppose \u03a5\u2208Rn\u03c5\u00d7dis a data matrix whose samples are from\nsubG (\u03a3), i.i.d. and\u02c6\u03a3 =1\nn\u03a5\u22a4\u03a5is an estimation of the second-moment ma-\ntrix. Let \u02dc\u03a3 = \u02c6\u03a3\u22121/2\u03a3\u02c6\u03a3\u22121/2, then \u02dc\u03bei=\u02c6\u03a3\u22121/2\u03bei\u223csubG (\u02dc\u03a3)and, with at least\nprobability 1\u22122\u03b7,\n\u2225\u02dc\u03bei\u22252\n2\u2264Tr(\u02dc\u03a3) + O(dlog(2n\u03be\n\u03b7))\u2264O(d(1 + log(2n\u03be\n\u03b7))). (12)\nProof 3. Combining the Theorem 1 and the n\u03beunion bound of the Lemma\n4, we know the averaged trace Tr(\u02dc\u03a3)tends to 1as the lower bound Land the\nupper bound Utending to 1. \u25a1\nRemark 1. TheCorollary 2 illustrates that no sample is truncated with\nhigh probability and shows that the public second-moment makes the trunca-\ntion radius without dependence on the private and a priori information.\n13\n\n--- Page 14 ---\n3.2. Differentially Private Second-moment Estimation\nWe propose the algorithm 2 to get a private second-moment matrix esti-\nmation based on transformed data. From Eq.(1), we know that the second-\nmoment matrix estimation of transformed data is related to the original data,\nso we should protect it by the Gaussian mechanism.\nAlgorithm 2 Differentially Private PMT Second-moment Estimation (DP-\nPMTSE)\n1:Input: Private dataset {\u03bei}n\u03be\ni=1, public second-moment \u02c6\u03a3 =\n1\nn\u03c5Pn\u03c5\ni=1\u03c5\u03c5\u22a4, parameters \u03c1,d,n\u03beand\u03b7.\n2:Truncating private data:\n{\u02dc\u03bei}n\u03be\ni=1=PMT ({\u03bei}n\u03be\ni=1,\u02c6\u03a3, d, n \u03be, \u03b7).\n3:Private parameter:\n\u03c3=2d(1+log(2n\u03be\n\u03b7))\n\u221a2\u03c1\u00b7n\u03be\n4:Gaussian mechanism: \u02dc\u03a3 =1\nn\u03bePn\u03be\ni=1\u02dc\u03bei\u02dc\u03bei\u22a4+G, where G\u223cGUE (\u03c32).\n5:Output: DP estimation \u02dc\u03a3.\nTheorem 2 (Privacy Guarantee) .The algorithm 2 satisfies \u03c1\u2212zCDP and\n,with at least probability 1\u2212\u03b7, the Gaussian random matrix is bounded by\n\u2225G\u22252\u2264O\u0010d3/2(1 + log(2n\u03be\n\u03b7)) log(1\n\u03b7)\n\u221a\u03c1\u00b7n\u03be\u0011\n. (13)\nProof 4. 1.Privacy . Given two neighboring data sets, the global l2-\nsensitivity of the truncation second-moment matrix is\n\n\n\n1\nn\u03be\u0010\n\u02dc\u03be\u02dc\u03be\u22a4\u2212\u02dc\u03be\u2032\u02dc\u03be\u2032\u22a4\u0011\n\n\nF\u22641\nn\u03be\u2225\u02dc\u03be\u22252\n2+1\nn\u03be\u2225\u02dc\u03be\u2032\u22252\n2\u22642d(1 + log(2n\u03be\n\u03b7))\nn\u03be,\nwhere \u02dc\u03beand\u02dc\u03be\u2032are the different samples in the neighboring data sets. From\nthe Gaussian mechanism, we get the privacy guarantee.\n2.Noisy matrix bound . It\u2019s a direct conclusion from the Lemma\n7. \u25a1\n14\n\n--- Page 15 ---\n3.3. Main Result\nWe propose the core algorithm 3 to get the DP-OLSE and give the the-\noretical error bound in Theorem 3 .\nAlgorithm 3 Differential Private PMT Ordinary Least Square Estimator\n(DP-PMTOLSE)\n1:Input: Private dataset {\u03bei= (Ai,yAi)\u22a4\u2208Rd+1}nA\ni=1, public second-\nmoments \u02c6\u03a3B=1\nnBB\u22a4Band \u02c6\u03c3B=q\n1\nnBPnB\ni=1y2\nBi. Parameters \u03c1,d,nAand\n\u03b7.\n2:Transforming data:\n{\u02dcAi}nA\ni=1=PMT ({Ai}nA\ni=1,\u02c6\u03a3B, d, n A, \u03b7).\n3:Transforming responses:\n{\u02dcyAi}nA\ni=1=PMT ({yAi}nA\ni=1,\u02c6\u03c32\nB,1, nA, \u03b7).\n4:Private parameter:\n\u03c31=2d(1+log(2nA\n\u03b7))\n\u221a2\u03c1\u00b7nA,\u03c32=2\u221a\nd(1+log(2nA\n\u03b7))\n\u221a2\u03c1nA.\n5:Gaussian mechanism: \u02dc\u03b2DP\nA=\u0010\n\u02dcA\u22a4\u02dcA\nnA+G\u0011\u22121\u0010\u02dcA\u22a4\u02dcyA\nnA+g\u0011\n, where G\u223c\nGUE (\u03c32\n1) and g\u223c N(0, \u03c32\n2I).\n6:Recover: \u02c6\u03b2DP\nA\u2190\u02c6\u03c3B\u00b7\u02c6\u03a3\u22121/2\nB\u00b7\u02dc\u03b2DP\nA.\n7:Output: DP estimator \u02c6\u03b2DP\nA.\nTheorem 3 (DP-PMTOLSE) .The algorithm 3 satisfies 2\u03c1\u2212zCDP . And\nfixed \u03b7 >0, with at least probability 1\u2212O(\u03b7), we can guarantee the transfor-\nmation OLSE\n\u02dc\u03b2Awithout truncation loss . (14)\nWhen\u221anB\u2265O(\u221a\nd+p\n2 log(1 /\u03b7))andnAmakesd3/2(1+log(2nA\n\u03b7)) log(1\n\u03b7)\n\u221a\u03c1\u00b7nAL(1\u2212O(\u221a\nd+\u221a\nlog(1/\u03b7)\u221anA))2\u2264\n1\n2, then, with at least probability 1\u2212O(\u03b7),\n\u2225\u02dc\u03b2DP\nA\u2212\u02dc\u03b2A\u22252\u2264O\u0010 d3/2\u2225\u03b2\u2225\u2225\u02c6\u03a3A\u2225\u2225\u02c6\u03a3\u22121/2\nB\u2225log(1\n\u03b7)\n\u02c6\u03c3B\u00b7\u221a\u03c1\u00b7nA\u00b7(1\u2212O(\u221a\nd+\u221a\nlog(1 /\u03b7)\u221anA))4\u00b7(1 + log(2nA\n\u03b7))\nL2\u0011\n.\n(15)\nMoreover, the accuracy of DP-PMTOLSE satisfies\n15\n\n--- Page 16 ---\n\u2225\u02c6\u03b2DP\nA\u2212\u02c6\u03b2A\u22252\u2264O\u0010d3/2\u2225\u03b2\u2225\u2225\u02c6\u03a3A\u2225\u2225\u02c6\u03a3\u22121\nB\u2225log(1\n\u03b7)\n\u221a\u03c1\u00b7nA\u00b7(1\u2212O(\u221a\nd+\u221a\nlog(1 /\u03b7)\u221anA))4\u00b7(1 + log(2nA\n\u03b7))\nL2\u0011\n.(16)\nwhere L=nB\n(\u221anB+O(\u221a\nd+q\n2 log(1\n\u03b7)))2.\nProof 5. 1.Privacy. \u02dc\u03a3A+Gsatisfies \u03c1\u2212zCDP from the Theorem 2 .\nWe mainly discuss the second term \u02dc\u03a3Ay+g. The sensitivity of \u02dc\u03a3Ayis\n\u2206\u02dc\u03a3Ay= max\n(\u02dcA,\u02dcyA)\u223c(\u02dcA\u2032,\u02dcy\u2032A\u2032)1\nnA\u2225\u02dcA\u22a4\u02dcyA\u2212\u02dcA\u2032\u22a4\u02dcy\u2032A\u2032\u22252\n(i)= max\n(\u02dcA,\u02dcyA)\n(\u02dcA\u2032,\u02dcy\u2032A\u2032)1\nnA\u2225\u02dcA\u22a4\u02dcyA\u2212\u02dcA\u2032\u22a4\u02dcy\u2032A\u2032\u22252\n\u2264max\n(\u02dcA,\u02dcyA)2\nnA\u2225\u02dcA\u22a4\u02dcyA\u22252\n\u2264max\n(\u02dcA,\u02dcyA)2\nnA\u2225\u02dcA\u22a4\u22252\u2225\u02dcyA\u22252\n\u22642\nnAr\nd(1 + log(2nA\n\u03b7))\u00b7(1 + log(2nA\n\u03b7))\n=O\u0010\u221a\nd(1 + log(2nA\n\u03b7))\nnA\u0011\n,\nwhere (\u02dcA,\u02dcyA)and(\u02dcA\u2032,\u02dcy\u2032A\u2032)are the different samples in (i). From the com-\nposition\n2.No truncation loss. From the Corollary 2 , with at least probability\n1\u22122\u03b7, we have no truncated transformation data \u02dcAi. Similarly, with at\nleast probability 1\u22122\u03b7, we have no truncated transformation data \u02dcyi. These\nguarantee, with at least probability 1\u22124\u03b7,\n\u02dc\u03b2A=\u0010\u02dcA\u22a4\u02dcA\nnA\u0011\u22121\u0010\u02dcA\u22a4\u02dcyA\nnA\u0011\n= (\u02dc\u03a3A)\u22121\u02dc\u03a3Ay, (17)\nis without loss from the truncation.\n16\n\n--- Page 17 ---\n3.Accuracy. Assume that \u02dc\u03b2Ais no truncated loss, then,\n\u2225\u02dc\u03b2DP\nA\u2212\u02dc\u03b2A\u22252=\u2225(\u02dc\u03a3A+G)\u22121(\u02dc\u03a3Ay+g)\u2212(\u02dc\u03a3A)\u22121\u02dc\u03a3Ay\u22252\n\u2264 \u2225(\u02dc\u03a3A+G)\u22121\u2212\u02dc\u03a3\u22121\nA\u22252| {z }\n(\u2217)\u2225\u02dc\u03a3Ay\u22252\n+\u2225(\u02dc\u03a3A+G)\u22121\u22252\u2225g\u22252| {z }\n(\u22c4),(18)\nwhere \u02dc\u03a3A=\u02dcA\u22a4\u02dcA\nnAand\u02dc\u03a3Ay=\u02dcA\u22a4\u02dcyA\nnA.\nThe first term (\u2217)is bounded as follows. From the Lemma 5 ,\n1\nL(1\u22121\u221a\nLO(q\nd\nnA+q\nlog(1 /\u03b7)\nnA))2\u22651\n\u03bbmin(\u02dc\u03a3A),\nwhere L=nB\n(\u221anB+O(\u221a\nd+q\n2 log(1\n\u03b7)))2. Since\u221anB\u2265O(\u221a\nd+p\n2 log(1 /\u03b7)), we\nhave1\u221a\nL= 1 + O(\u221a\nd+\u221a\n2 log(1 /\u03b7)\u221anB)\u2264O(1)and get the simplified inequality\n1\nL(1\u2212O(q\nd\nnA+q\nlog(1 /\u03b7)\nnA))2\u2265 \u2225\u02dc\u03a3\u22121\nA\u22252. (19)\nSo if\u2225\u02dc\u03a3\u22121\nA\u2225\u2225G\u2225 \u2264\u2225G\u2225\nL(1\u2212O(\u221a\nd+\u221a\nlog(1/\u03b7)\u221anA))2\u22641\n2, we get the following result from\ntheLemma 6\n(\u2217)\u2264\u2225G\u2225\u2225\u02dc\u03a3\u22121\nA\u22252\n1\u2212 \u2225\u02dc\u03a3\u22121\nA\u2225\u2225G\u2225\n\u22642\u2225G\u2225\u2225\u02dc\u03a3\u22121\nA\u22252\n(i)\n\u22642\u2225G\u2225\nL2(1\u2212O(q\nd+log(1 /\u03b7)\nnA))4,(20)\nwhere (i)from Eq.(19). From the Theorem 2 , the sufficient condition for\n\u2225\u02dc\u03a3\u22121\nA\u2225\u2225G\u2225 \u2264\u2225G\u2225\nL(1\u2212O(\u221a\nd+\u221a\nlog(1/\u03b7)\u221anA))2\u22641\n2to hold is\nd3/2(1 + log(2nA\n\u03b7)) log(1\n\u03b7)\nL(1\u2212O(q\nd+log(1 /\u03b7)\nnA))2\u221a\u03c1\u00b7nA\u22641\n2, (21)\n17\n\n--- Page 18 ---\nwhere L=nB\n(\u221anB+O(\u221a\nd+q\n2 log(1\n\u03b7)))2. Then, using the Theorem 2 again, we get\n(\u2217)\u2264O\u0010d3/2\n\u221a\u03c1\u00b7nA(1 + log(2nA\n\u03b7)) log(1\n\u03b7)\nL2(1\u2212O(q\nd+log(1 /\u03b7)\nnA))4\u0011\n, (22)\nThe second term (\u22c4)is similar to the analysis of the first,\n(\u22c4)(i)\n\u22642\u2225\u02dc\u03a3\u22121\nA\u2225\u2225g\u2225, (23)\nwhere (i)is from the nAassumption and the Lemma 6 . Since g\u223c N(0, \u03c32\n2)\nwith\u03c32=O\u0010\u221a\nd(1+log(2nA\n\u03b7))\n\u221a\u03c1nA\u0011\nand the Lemma 8 , we know\n\u2225g\u22252\u2264O(\u03c32(r\nd+ log(1\n\u03b7)))\n\u2264O\u0010d(1 + log(2nA\n\u03b7))\n\u221a\u03c1nA\u0011\n,(24)\nSo\n(\u22c4)\u22642\u2225\u02dc\u03a3\u22121\nA\u2225\u2225g\u2225 \u2264O\u0010 d(1 + log(2nA\n\u03b7))\n\u221a\u03c1nAL(1\u2212O(q\nd+log(1 /\u03b7)\nnA))2\u0011\n, (25)\nUnder the event of no truncated loss, combining Eq.(22) and Eq.(25), we get\n\u2225\u02dc\u03b2DP\nA\u2212\u02dc\u03b2A\u22252\u2264O\u0010d3/2\n\u221a\u03c1\u00b7nA(1 + log(2nA\n\u03b7)) log(1\n\u03b7)\nL2(1\u2212O(q\nd+log(1 /\u03b7)\nnA))4\u0011\n\u2225\u02dc\u03a3Ay\u22252\n+O\u0010 d(1 + log(2nA\n\u03b7))\n\u221a\u03c1nAL(1\u2212O(q\nd+log(1 /\u03b7)\nnA))2\u0011\n(i)\n\u2264O\u0010d3/2\n\u221a\u03c1\u00b7nA(1 + log(2nA\n\u03b7)) log(1\n\u03b7)\nL2(1\u2212O(q\nd+log(1 /\u03b7)\nnA))4\u0011\n\u2225\u02dc\u03a3Ay\u22252(26)\nwhere (i)omits the second term (\u22c4), because its bound order is smaller than\nthe first term (\u2217)generally.\n18\n\n--- Page 19 ---\nSince \u2225\u02dc\u03a3Ay\u22252\u2264\u2225\u02c6\u03a3\u22121/2\nB\u22252\n\u02c6\u03c3B\u2225\u02c6\u03a3Ay\u22252\u2264\u2225\u02c6\u03a3\u22121/2\nB\u22252\n\u02c6\u03c3BO(\u2225\u02c6\u03a3A\u22252\u2225\u03b2\u22252), we have, with\nprobability 1\u2212O(\u03b7),\n\u2225\u02dc\u03b2DP\nA\u2212\u02dc\u03b2A\u22252\u2264O\u0010d3/2\u2225\u02c6\u03a3\u22121/2\nB\u22252\u2225\u02c6\u03a3A\u22252\u2225\u03b2\u22252log(1\n\u03b7)(1 + log(2nA\n\u03b7))\n\u02c6\u03c3B\u221a\u03c1\u00b7nA\u00b7L2(1\u2212O(q\nd+log(1 /\u03b7)\nnA))4\u0011\n. (27)\nThen, the upper bound of recovering OLSE is\n\u2225\u02c6\u03c3B\u02c6\u03a3\u22121/2\nB\u02dc\u03b2DP\nA\u2212\u02c6\u03b2A\u22252=\u2225\u02c6\u03c3B\u02c6\u03a3\u22121/2\nB\u02dc\u03b2DP\nA\u2212\u02c6\u03c3B\u02c6\u03a3\u22121/2\nB\u02dc\u03b2A\u22252\n\u2264 |\u02c6\u03c3B|\u2225\u02c6\u03a3\u22121/2\nB\u22252\u2225\u02dc\u03b2DP\nA\u2212\u02dc\u03b2A\u22252\n\u2264O\u0010d3/2\u2225\u02c6\u03a3\u22121\nB\u22252\u2225\u02c6\u03a3A\u22252\u2225\u03b2\u22252log(1\n\u03b7)(1 + log(2nA\n\u03b7))\n\u221a\u03c1\u00b7nA\u00b7L2(1\u2212O(q\nd+log(1 /\u03b7)\nnA))4\u0011\n.(28)\nFinally, we get the conclusion that, with probability 1\u2212O(\u03b7), the accuracy\nof DP-PMTOLSE satisfies\n\u2225\u02c6\u03b2DP\nA\u2212\u02c6\u03b2A\u22252\u2264O\u0010d3/2\u2225\u02c6\u03a3\u22121\nB\u22252\u2225\u02c6\u03a3A\u22252\u2225\u03b2\u22252log(1\n\u03b7)(1 + log(2nA\n\u03b7))\n\u221a\u03c1\u00b7nA\u00b7L2(1\u2212O(q\nd+log(1 /\u03b7)\nnA))4\u0011\n. (29)\n\u25a1\n3.4. The Advantages of Our Method\nThere is a necessary comparison to a naive truncation method and private-\ndata-only SSP linear regression, where we use the trace of private second-\nmoment estimation as a truncation radius, seeing the algorithm 4.\nTheorem 4 (DP-OLSE) .The algorithm 4 gives a DP-OLSE satisfying 2\u03c1\u2212\nzCDP . And fixed \u03b7 >0, with at least probability 1\u2212O(\u03b7), when the number\nof private data nAmakesd3/2(Tr(\u02c6\u03a3B)+log(2nA\n\u03b7)) log(1\n\u03b7)\n\u221a\u03c1\u00b7nA\u00b7\u03bbmin(\u02c6\u03a3A)\u22641\n2, then\n\u2225\u02c6\u03b2DP\nA\u2212\u02c6\u03b2A\u22252\u2264O\u0010d3/2\u2225\u03b2\u22252\u03ba(\u02c6\u03a3A) log(1\n\u03b7)\n\u221a\u03c1\u00b7nA\u00b7\u0010log(2nA\n\u03b7)\n\u03bbmin(\u02c6\u03a3A)+ \u00af\u03ba(\u02c6\u03a3A)\u0011\u0011\n(30)\nwhere \u02c6\u03a3A=A\u22a4A\nnA,Tr(\u02c6\u03a3A) =1\ndPd\ni=1\u03bbi(\u02c6\u03a3A),and \u00af\u03ba(\u02c6\u03a3A) =1\ndPd\ni=1\u03bbi(\u02c6\u03a3A)\n\u03bb1(\u02c6\u03a3A), \u03bbd(\u02c6\u03a3A)\u2265\n\u00b7\u00b7\u00b7 \u2265 \u03bb1(\u02c6\u03a3A).\n19\n\n--- Page 20 ---\nRemark 2. The proof of this theorem is similar to the DP-PMTOLSE. So\nwe omit it here and show the critical conclusion.\nThere are two theoretical advantages worth our attention and comparison\non the DP-PMTOLSE ( Theorem 3 ) and the DP-OLSE ( Theorem 4 ). For\nconvenience and in the case of large samples, we replace estimations \u02c6\u03a3Aand\n\u02c6\u03a3Bwith the real second-moment \u03a3 in the two results.\n1.Strong robustness. Considering the inverse stable condition about\nthe number of private data nAinTheorem 4 ,\nd3/2(Tr(\u03a3) + log(2nA\n\u03b7)) log(1\n\u03b7)\n\u221a\u03c1\u00b7nA\u00b7\u03bbmin(\u03a3)=d3/2log(1\n\u03b7)\n\u221a\u03c1\u00b7nA\u0010\n\u00af\u03ba(\u03a3) + \u2225\u03a3\u22121\u2225log(2nA\n\u03b7)\u0011\n,\n(31)\nwhere \u00af \u03ba(\u03a3) is the averaged condition number of the second-moment\nmatrix, \u00af \u03ba(\u03a3) =1\ndPd\ni=1\u03bbi\n\u03bb1. Obviously, when the second-moment is ill-\nconditioned matrix with large \u00af \u03ba(\u03a3) and \u2225\u03a3\u22121\u2225, the more private data\nis needed to guarantee the inverse stable condition Eq.(31) \u22641\n2.\nTheorem 3 eliminates the impact of the unknown second-moment,\nd3/2(1 + log(2nA\n\u03b7)) log(1\n\u03b7)\n\u221a\u03c1\u00b7nAL(i)=d3/2log(1\n\u03b7)\n\u221a\u03c1\u00b7nA\u0010\n1 + log(2nA\n\u03b7)\u0011\n,\nwhere the Eq.( i) is because of L=nB\n(\u221anB+O(\u221a\nd+q\n2 log(1\n\u03b7)))2tending to 1\nfast as nBincreases. Hence, we decrease the consumption of private\ndata meanwhile strengthening the regression robustness and utility.\n2.Better error bound. The rate of convergence in the Theorem 4\ndepends on the averaged condition number \u00af \u03ba(\u03a3) and \u2225\u03a3\u22121\u2225,\n\u2225\u02c6\u03b2DP\nA\u2212\u02c6\u03b2A\u22252\u2264O\u0010d3/2\u2225\u03b2\u2225\u03ba(\u03a3) log(1\n\u03b7)\n\u221a\u03c1\u00b7nA\u00b7\u0010\nlog(2nA\n\u03b7)\u2225\u03a3\u22121\u2225+ \u00af\u03ba(\u03a3)\u0011\u0011\n.\nTheorem 3 shows our method effectively eliminates the impacts of\nthe unknown second-moment \u03a3. Especially when the second-moment\nmatrix is ill-conditioned, our method has an better convergence prop-\nerty.\n\u2225\u02c6\u03b2DP\nA\u2212\u02c6\u03b2A\u22252\u2264O\u0010d3/2\u2225\u03b2\u2225\u03ba(\u03a3) log(1\n\u03b7)\n\u221a\u03c1\u00b7nA\u00b7\u0010log(2nA\n\u03b7)\nL2+1\nL2\u0011\u0011\n(i)\n\u2264O\u0010d3/2\u2225\u03b2\u2225\u03ba(\u03a3) log(1\n\u03b7)\n\u221a\u03c1\u00b7nA\u0000\nlog(2nA\n\u03b7) + 1\u0001\u0011\n,\n20\n\n--- Page 21 ---\nwhere ( i) is because of Ltending to 1, as nB\u21921.\nAlgorithm 4 Differential Private Ordinary Least Square Estimator (DP-\nOLSE)\n1:Input: Private dataset {\u03bei= (Ai,yAi)\u22a4\u2208Rd+1}nA\ni=1,\u02c6\u03a3A=1\nnAA\u22a4Aand\n\u02c6\u03c3A=q\n1\nnAPnA\ni=1y2\nAiParameters \u03c1,d,nAand\u03b7.\n2:Truncate data: for every i= 1, ..., n A,\n3:if\u2225Ai\u22252\u2265q\nTr(\u02c6\u03a3A) +dlog(2n\u03be\n\u03b7)then\n4:Ai=q\nTr(\u02c6\u03a3A) +dlog(2n\u03be\n\u03b7)\u00b7Ai\n\u2225Ai\u22252;\n5:else\n6:Aiis itself.\n7:end if\n8:Truncate responses: for every i= 1, ..., n A,\n9:if\u2225yAi\u22252\u2265q\n\u02c6\u03c32\nA+ log(2n\u03be\n\u03b7)then\n10: yAi=q\n\u02c6\u03c32\nA+ log(2n\u03be\n\u03b7)\u00b7yAi\n\u2225yAi\u22252;\n11:else\n12: yAiis itself.\n13:end if\n14:Private parameter:\n\u03c31=2(Tr( \u02c6\u03a3A)+dlog(2nA\n\u03b7))\n\u221a2\u03c1\u00b7nA,\u03c32=2q\n(Tr(\u02c6\u03a3A)+dlog(2nA\n\u03b7))\u00b7(\u02c6\u03c32\nA+log(2nA\n\u03b7))\n\u221a2\u03c1nA.\n15:Gaussian mechanism: \u02c6\u03b2DP\nA=\u0010\nA\u22a4A\nnA+G\u0011\u22121\u0010\nA\u22a4yA\nnA+g\u0011\n, where G\u223c\nGUE (\u03c32\n1) and g\u223c N(0, \u03c32\n2I).\n16:Output: DP OLSE \u02c6\u03b2DP\nA.\n4. Experiment\nIn this section, we evaluate our method in the synthetic data and the real-\nworld dataset. The first subsection is about the experiments in the synthetic\ndata and the second subsection is about the real-world dataset.\n4.1. Synthetic data\nWe design the linear model to generate the response data yand the fea-\nture data Xwith the dimension d= 10. The features\u2019 distribution is N(\u00b5,\u03a8),\n21\n\n--- Page 22 ---\nthe noise \u03c9\u223c N(0,(0.05)2) and the parameter \u03b2\u223c N(0,I). Namely,\ny=X\u03b2+\u03c9.\nWe generate the private dataset ( AnA\u00d7d,yA) and the public dataset ( BnB\u00d7d,yB),\nnB= 20. Typically, we denote the second moment of feature data as \u03a3 =\n\u03a8 + \u00b5\u00b5T, where \u00b5\u00b5Tactually results in \u03a3 with a large max eigenvalue\nand a worse ill-condition. All experiments set the probability parameter\nas\u03b7= 0.05. All results are averaged over 300 independent trials.\nFirstly, we show the averaged l2norm error between the true parameters\nand the OLS estimations of the algorithm 4(DP-OLSE) and the algorithm\n3(DP-PMTOLSE) when we take different privacy budgets \u03c1={2,10}and\ndifferent numbers of private data nA={3000,3500,4000,5000,6000,8000,10000}.\nFigure 1: Simulation with different privacy budgets and numbers of private data.\nFigure 1 shows the average errors (lines) and their standard deviations\n(shaded areas). Overall, larger privacy parameters\u2014which imply weaker\nprivacy protection and less noise\u2014result in more accurate DP estimates\nthat converge more reliably to the true model parameters. Notably, even\nwith the largest privacy budget, DP-OLSE produces higher errors than DP-\nPMTOLSE with smaller budget, despite the latter ensuring stronger privacy\nand injecting more noise. Furthermore, DP-OLSE demonstrates significantly\n22\n\n--- Page 23 ---\npoorer robustness compared to DP-PMTOLSE. These results highlight that\nour proposed method achieves superior accuracy and robustness over stan-\ndard DP ordinary least squares regression.\nSecondly, we explore the impacts of the different numbers of public data\nand the different privacy budgets to DP-PMTOLSE. We set the privacy\nbudgets to be \u03c1={10,15,20,25}and add the public sample one-by-one into\nthe public dataset.\nFigure 2: Simulation with different privacy budgets and numbers of public data.\nFigure 2 illustrates that DP-PMTOLSE with a good performance needs a\nnecessary condition where the estimation of the public second-moment matrix\nrequires the number of public data points to exceed the feature dimension.\nParticularly, in this experiment, when the number of public data nB= 11 >\nd= 10, the errors tend to be small and stable.\nFinally, we investigate the effects of varying the number of public samples\nnBand private samples nAon the performance of DP-PMTOLSE. We set the\nprivacy budgets to be nA={5000,10000 ,15000 ,20000 ,25000}and add the\npublic sample one-by-one into the public dataset.\n23\n\n--- Page 24 ---\nFigure 3: Simulation with different numbers of public data and private data.\nFigure 3 demonstrates that increasing the number of public data points\nnBhelps reduce the estimation error of DP-PMTOLSE, even when the pri-\nvate dataset is small. The improvement in robustness appears to be lim-\nited. When the public second-moment matrix is estimated from sufficiently\nmany public data, the reduction in the averaged condition number tends to\nplateau, resulting in only marginal gains in robustness. In this simulation,\nthe averaged condition number and the minimum eigenvalue of the estimated\nprivate second-moment matrix are \u00af \u03ba(\u02c6\u03a3pri) = 15 .32 and \u03bbmin(\u02c6\u03a3pri) = 0 .17. Af-\nter transformation, the corresponding averaged condition number of \u00af \u03ba(\u02c6\u03a3tran)\nare reduced to 2 .7 and 1 .6 for nB= 40 and nB= 135, respectively. The\nassociated minimum eigenvalues increase to 0 .45 and 0 .7, respectively.\n4.2. Real-world data\nThe White-wine Quality [30] is a real-world dataset used for modeling\nwine quality based on physicochemical tests, including 4898 samples with\n11 continuous features and one target (wine quality, scored between 0 and\n10). In order to stabilize the experimental values, we normalize the dataset\nand separate it as 249 public data and 4649 private data. The averaged\ncondition number of second-moment matrix estimation of private data is\n\u00af\u03ba(\u02c6\u03a3pri) = 68 .4. Using all public data points, we estimate the second-moment\n24\n\n--- Page 25 ---\nmatrix \u02c6\u03a3pub; the corresponding transformed second-moment matrix \u02c6\u03a3tranhas\nan averaged condition number of \u00af \u03ba(\u02c6\u03a3tran) = 1 .6. We repeat 300 trials and\nshow the averaged error \u2225\u02c6\u03b2DP\nA\u2212\u02c6\u03b2A\u22252between DP estimations and non-DP\nestimation from the entire private dataset.\nWe show the averaged l2norm error between the true parameters and\nthe OLS estimations of the algorithm 4(DP-OLSE) and the algorithm 3(DP-\nPMTOLSE) when we take different privacy budgets \u03c1={5,50,500}and\ndifferent numbers of private data nA={3000,3500,3800,4000,4200,4649}.\nFigure 4: Real data with different privacy budgets and numbers of private data.\nFigure 4 shows the average errors (lines) and their standard deviations\n(shaded areas). In the real dataset, the DP-PMTOLSE achieves a significant\nperformance compared to the DP-OLSE. The DP-PMTOLSE with strong\nprivacy protection, \u03c1= 5, still has lower errors and better robustness than\nthe DP-OLSE with a weak privacy protection, \u03c1= 500. That is due to\nthe ill-conditioned second-moment matrix in the real-world dataset whose\n\u00af\u03ba(\u02c6\u03a3pri) = 68 .4. But \u00af \u03ba(\u02c6\u03a3tran) = 1 .6 is a significant improvement.\nFinally, we explore the impacts of the different numbers of public data\nand the different privacy budgets to DP-PMTOLSE in the real-world dataset.\nWe set the privacy budgets to be \u03c1={5,10,20,30}and add the public\nsample one-by-one into the public dataset. Figure 5 illustrates that in the\n25\n\n--- Page 26 ---\nreal-world dataset, the increase of public data also helps the improvement of\nDP-PMTOLSE.\nFigure 5: Real data with different privacy budgets and numbers of public data.\n5. Conclusion and Future Work\nIn this work, we propose a method which transforms private data for dif-\nferentially private linear regression by utilizing the second-moment of public\ndata. We demonstrate the difference between our method and the conven-\ntional SSP regression and theoretically give an error bound between our\nmethod and the non-DP. These theoretical findings provide an explanation\nfor the improved accuracy and robustness of our approach. Our experiments\nshow that the second-moment matrix estimation using tiny amounts of pub-\nlic data can greatly enhance DP OLSE, and we validate the methods on both\nsynthetic and real datasets. Future research could examine ways to improve\nDP algorithms by using public information even more and apply this strategy\nto other DP machine learning methods.\n26\n\n--- Page 27 ---\nReferences\n[1] C. Dwork, A. Roth, et al., The algorithmic foundations of differential\nprivacy, Foundations and Trends \u00aein Theoretical Computer Science\n9 (3\u20134) (2014) 211\u2013407.\n[2] I. Dinur, K. Nissim, Revealing information while preserving privacy,\nin: Proceedings of the twenty-second ACM SIGMOD-SIGACT-SIGART\nsymposium on Principles of database systems, 2003, pp. 202\u2013210.\n[3] L. Wasserman, S. Zhou, A statistical framework for differential privacy,\nJournal of the American Statistical Association (2010) 375\u2013389.\n[4] C. Dwork, A. Smith, T. Steinke, J. Ullman, S. Vadhan, Robust trace-\nability from trace amounts, in: 2015 IEEE 56th Annual Symposium on\nFoundations of Computer Science, IEEE, 2015, pp. 650\u2013669.\n[5] G. Kamath, J. Li, V. Singhal, J. Ullman, Privately learning high-\ndimensional distributions, in: Conference on Learning Theory, PMLR,\n2019, pp. 1853\u20131902.\n[6] P. Jain, A. G. Thakurta, (near) dimension independent risk bounds for\ndifferentially private learning, in: International Conference on Machine\nLearning, PMLR, 2014, pp. 476\u2013484.\n[7] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Tal-\nwar, L. Zhang, Deep learning with differential privacy, in: Proceedings\nof the 2016 ACM SIGSAC conference on computer and communications\nsecurity, 2016, pp. 308\u2013318.\n[8] A. Blanco-Justicia, D. S\u00b4 anchez, J. Domingo-Ferrer, K. Muralidhar, A\ncritical review on the use (and misuse) of differential privacy in machine\nlearning, ACM Computing Surveys 55 (8) (2022) 1\u201316.\n[9] L. Zhang, H. Zhang, Shuffle private decentralized convex optimization,\nIEEE Transactions on Information Forensics and Security 19 (2024)\n5842\u20135851. doi:10.1109/TIFS.2024.3405183.\n[10] C. Ferrando, J. Gillenwater, A. Kulesza, Combining public and private\ndata, arXiv preprint arXiv:2111.00115 (2021).\n27\n\n--- Page 28 ---\n[11] A. Bie, G. Kamath, V. Singhal, Private estimation with public data,\nAdvances in neural information processing systems 35 (2022) 18653\u2013\n18666.\n[12] M. Nasr, S. Mahloujifar, X. Tang, P. Mittal, A. Houmansadr, Effectively\nusing public data in privacy preserving machine learning, in: Interna-\ntional Conference on Machine Learning, PMLR, 2023, pp. 25718\u201325732.\n[13] P. Kairouz, M. R. Diaz, K. Rush, A. Thakurta, (nearly) dimension in-\ndependent private erm with adagrad rates via publicly estimated sub-\nspaces, in: Conference on Learning Theory, PMLR, 2021, pp. 2717\u20132746.\n[14] E. Amid, A. Ganesh, R. Mathews, S. Ramaswamy, S. Song, T. Steinke,\nV. M. Suriyakumar, O. Thakkar, A. Thakurta, Public data-assisted mir-\nror descent for private model training, in: International Conference on\nMachine Learning, PMLR, 2022, pp. 517\u2013535.\n[15] Z. Ji, C. Elkan, Differential privacy based on importance weighting,\nMachine learning 93 (2013) 163\u2013183.\n[16] A. Nandi, R. Bassily, Privately answering classification queries in the\nagnostic pac model, in: Algorithmic Learning Theory, PMLR, 2020, pp.\n687\u2013703.\n[17] R. Bassily, S. Moran, A. Nandi, Learning from mixtures of private and\npublic populations, Advances in neural information processing systems\n33 (2020) 2947\u20132957.\n[18] T. Liu, G. Vietri, T. Steinke, J. Ullman, S. Wu, Leveraging public data\nfor practical private query release, in: International Conference on Ma-\nchine Learning, PMLR, 2021, pp. 6968\u20136977.\n[19] T. T. Cai, Y. Wang, L. Zhang, The cost of privacy: Optimal rates\nof convergence for parameter estimation with differential privacy, The\nAnnals of Statistics 49 (5) (2021) 2825\u20132850.\n[20] R. Arora, R. Bassily, C. Guzm\u00b4 an, M. Menart, E. Ullah, Differentially\nprivate generalized linear models revisited, Advances in neural informa-\ntion processing systems 35 (2022) 22505\u201322517.\n28\n\n--- Page 29 ---\n[21] K. Chaudhuri, C. Monteleoni, A. D. Sarwate, Differentially private em-\npirical risk minimization., Journal of Machine Learning Research 12 (3)\n(2011).\n[22] D. Kifer, A. Smith, A. Thakurta, Private convex empirical risk mini-\nmization and high-dimensional regression, in: Conference on Learning\nTheory, JMLR Workshop and Conference Proceedings, 2012, pp. 25\u20131.\n[23] Y.-X. Wang, Revisiting differentially private linear regression: opti-\nmal and adaptive prediction & estimation in unbounded domain, arXiv\npreprint arXiv:1803.02596 (2018).\n[24] O. Sheffet, Differentially private ordinary least squares, in: International\nConference on Machine Learning, PMLR, 2017, pp. 3105\u20133114.\n[25] J. Milionis, A. Kalavasis, D. Fotakis, S. Ioannidis, Differentially private\nregression with unbounded covariates, in: International Conference on\nArtificial Intelligence and Statistics, PMLR, 2022, pp. 3242\u20133273.\n[26] M. Bun, T. Steinke, Concentrated differential privacy: Simplifications,\nextensions, and lower bounds, in: Theory of cryptography conference,\nSpringer, 2016, pp. 635\u2013658.\n[27] R. Vershynin, Introduction to the non-asymptotic analysis of random\nmatrices, arXiv preprint arXiv:1011.3027 (2010).\n[28] T. Tao, Topics in random matrix theory, Vol. 132, American Mathemat-\nical Soc., 2012.\n[29] B. Laurent, P. Massart, Adaptive estimation of a quadratic functional\nby model selection, Annals of statistics (2000) 1302\u20131338.\n[30] Wine quality, UCI Machine Learning Repository, DOI:\nhttps://doi.org/10.24432/C56S3T (2009).\n29",
  "project_dir": "artifacts/projects/enhanced_stat.ML_2508.18037v1_Enhancing_Differentially_Private_Linear_Regression",
  "communication_dir": "artifacts/projects/enhanced_stat.ML_2508.18037v1_Enhancing_Differentially_Private_Linear_Regression/.agent_comm",
  "assigned_at": "2025-08-26T21:02:36.898781",
  "status": "assigned"
}